{"cells":[{"metadata":{},"cell_type":"markdown","source":"![Caption for the picture.](https://images.app.goo.gl/rcco9AayW5Xh6HWh9)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**Table Of Content**\n    1. Import Data and Data Structure\n         1.1 Import Data\n         1.2 Overview of Data Structure\n    2. Data visualization and Missing Values\n        2.1 Overview of Relation\n        2.2 Visualization of Features\n        2.3 Missing Values\n    3. Feature Engineering and Statistical Analysis\n        3.1 Features Generation\n        3.2 Statistical Analysis and Feature Selection\n        \n  \n    4. Modelling\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":" # 1. Import and Read Data","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## 1.1 Import Data","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# read the data\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom mpl_toolkits import mplot3d\nfrom matplotlib import cm\nimport seaborn as sns\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_absolute_error\n\n\n\nfile_path1 = \"../input/titanic/train.csv\"\nfile_path2= \"../input/titanic/test.csv\"\ndata = pd.read_csv(file_path1)\ntest_data=pd.read_csv(file_path2)\ndata.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 1.2 Overview of Data structure \n> Find out the data structure and type","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# show the first five line of data\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# show the data struture and type\ndata.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are totally 11 variables and we can divide them into 3 types: categorical variable, numeric variables and text variable:\n### 1.2.1 Categorical Variable\n\n* **Pclass **:\n>         1=1st  \n>         2=2st             \n>         3=3st \n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data.Pclass.value_counts()\n#s1=data.groupby('Pclass').apply(lambda df: df.loc[df.Survived==0].Survived.value_counts())\n#s2=data.groupby('Pclass').Survived.count()\ns1=data.groupby('Pclass').apply(lambda df: df.Survived.value_counts()/len(df)) \ns2=data.groupby('Pclass').apply(lambda df: df.Survived.value_counts()) \npd.concat([s2,s1],axis=1,keys=['Survived/Death count','Survived/Death Rate'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The above table shows the Survival rate for each Pclass which is an decreasing trend: 0.629 > 0.472 > 0.242 as 'Pclass' value increasing. Also Pclass=3 has the highest death population. Pclass will be an important feature for Survived prediction.\n* **Sex**:\n>         Male \n>         Female  \n\nMost of the passengers are male and female passenger was less than 50%","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data.Sex.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* **Cabin**: \n\nThe cabin number is a character followed by a number and there are 147 different cabin number in train dataset and 76 different numbe in test dataset. Now we group these Cabin number by their first character.\n\n\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data.Cabin.value_counts()\ntest_data.Cabin.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.Cabin.str[0].value_counts()\ntest_data.Cabin.str[0].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we can get all the types of Cabin are starting with these character:\n>         A \n>         B  \n>         C \n>         D  \n>         E \n>         F  \n>         G \n>         T  \n\n* Embarked (Port of Embarkation):\n>         S \n>         C  \n>         Q \n\n------------\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"s3=pd.Series(copy1.Embarked.value_counts())\n\npd.concat([s3,s3/889],axis=1,keys=['count(Embarked)','portion'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Numeric Variables\n* PassengerId (Uniquely define a passenger)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data.PassengerId.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There's 891 person in train dataset and each person has a unique PassengerId. However, PassengerId is a numeric value and there's a pretty small correlation (-0.05) between 'Survived' and 'PassengerId'. We may not consider this feature in the prediction modelling.\n\n\n* Age\n* SibSp (# of siblings / spouses aboard the Titanic)\n* Parch (# of parents / children aboard the Titanic)\n* Fare (the ticket price)\n\n-------\n### Text Variable\n* Name\n* Ticket\n----------","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"-------\n# 2. Data Visualization and Missing Values\n## 2.1 Categorical Encoding\nApply the categorical encoding method to the categorical variable and visualize their relation\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"copy1=data.copy()\ncopy2=test_data.copy()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n### Sex\n* Convert Sex into categorical variable by applying Label-Encoding","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"label_encoder=LabelEncoder()\ncopy1['new_Sex']=label_encoder.fit_transform(copy1['Sex'])\ncopy2['new_Sex']=label_encoder.transform(copy2['Sex'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Cabin\n* Oragnize the Cabin data by extracting their first character (A, B, C, D, E, F, G, T)\n* Treat them as categorical variable","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"copy1['new_Cabin']=data['Cabin'].str[0]\ns1=pd.Series(copy1.new_Cabin.value_counts())\ns2=s1/91\npd.concat([s1,s2],axis=1,keys=['count(new_cabin)','portion'])\n# Copy the test dataset and add a new column","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ncopy2['new_Cabin']=test_data['Cabin'].str[0]\ns3=pd.Series(copy2.new_Cabin.value_counts())\npd.concat([s3,s3/76],axis=1,keys=['count(new_Cabin)','portion'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* new_Cabin='C' has the highest portion in both dataset\n* Treat them as categorical variable by applying Label_Encoding\n* However, 'new_Cabin' contains missing value (null)\n* Regard all the missing value as new_Cabin='Z' for now and leave this problem to next part","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"copy1['new_Cabin']=copy1['new_Cabin'].fillna(\"Z\")\ncopy2['new_Cabin']=copy2.new_Cabin.fillna(\"Z\")\n\nlabel_encoder=LabelEncoder()\ncopy1['new_Cabin']=label_encoder.fit_transform(copy1['new_Cabin'])\ncopy2['new_Cabin']=label_encoder.transform(copy2['new_Cabin'])\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Embarked\n* Embarked also contains missing value\n* Regard all the missing value as Embarked='N'\n* Transfer Embarked into integer by applying label encoder","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"copy1['new_Embarked'] = copy1['Embarked'].fillna(\"N\")\n\nlabel_encoder=LabelEncoder()\ncopy1['new_Embarked']=label_encoder.fit_transform(copy1['new_Embarked'])\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"----\n## 2.2 Overview of Relation\nVisualize the relation between feature by correlation Heatmap and pairs plot","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"* Apply the pairs plot","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.pairplot(data,hue='Pclass', diag_kws={'bw':0.1}, palette=\"husl\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Apply the correlation Heatmap","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# the correlation matrix\nfeatures=['PassengerId','Survived','Pclass','new_Sex','Age','SibSp','Parch','Fare','new_Cabin','new_Embarked']\ncorr=copy1[features].corr() \n# mask the upper triangle\nsns.set(style=\"white\")\nplt.figure(figsize=(11,7))\nmask=np.triu(np.ones_like(corr,dtype=np.bool))\n# colour\ncmap=sns.diverging_palette(240,10,n=9)\n# annot to display the value\nsns.heatmap(corr,annot=True,mask=mask,cmap='RdYlBu',linewidths=0.6)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Some of the features are highly correlated with Survived\n* Apply the pairs plot on those features","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"------\n## 2.2 Visualization of Features\nSome of the features are highly correlated with survival rate or with each other. Data visualization can help us find out the pattern behind them.\n\n### Gender, Age and Survived\n* Firstly, Consider the Age structure of each Gender \n*  We can discover that the Age structure of male and female are similar","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"s1=copy1.loc[copy1.Sex=='female'].Age.describe()\ns2=copy1.loc[copy1.Sex=='male'].Age.describe()\npd.concat([s1,s2],axis=1,keys=['Age|Sex=female','Age|Sex=male'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* The correlation Heatmap shows that Gender is highly correlated to 'Survived'\n* Calculate the survival rate for men and women","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"s1=copy1.groupby('Sex').apply(lambda df: df.Survived.value_counts())\ns2=copy1.groupby('Sex').apply(lambda df: df.Survived.value_counts()/len(df))\ns3=pd.concat([s1,s2],axis=1,keys=['count(Survival/Death)','Survival/Death rate'])\ns3","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#fig, ax = plt.subplots(figsize=(12,5),ncols=2)\n#d1=copy1.loc[copy1.Sex=='male']\n#d2=copy1.loc[copy1.Sex=='female'].Survived.value_counts()\n\n#d1=copy1.loc[copy1.Sex=='female']\n#f=['Survived']\n#X=d1[f].Survived.value_counts()\n\nsns.set(style=\"whitegrid\")\nax1=sns.barplot(y=s3.index, x =s3['count(Survival/Death)'],linewidth=2.5,facecolor=(1,1,1,0),errcolor=\"1\", edgecolor=\".1\")\nylabels = ['(female, survived)','(female, Dead)', '(male, Dead)', '(male, survived)']\nax1.set_yticklabels(ylabels)\ni=0\nlist1=s3['Survival/Death rate']\nfor p in ax1.patches:\n    label = list1[i]*100\n    i=i+1\n    plt.text(-36+p.get_width(), p.get_y()+0.55*p.get_height(),\n             str('{:1.2f}'.format(label))+'%',\n             ha='center', va='center')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Obviously, survival rate of female is much higher than male\n* Consider Sex vs. Age vs. Survival","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set(style=\"darkgrid\")\nf, (ax1,ax2) = plt.subplots(figsize=(18,7),ncols=2)\ns1= copy1.loc[copy1.Sex=='female']\nax1 = sns.distplot(a=s1.Age,bins=34, kde=False, \n                  hist_kws={\"rwidth\":1,'edgecolor':'black', 'alpha':1.0},color='azure',label=\"Age\",ax=ax1)\ns2= s1.loc[(s1.Survived==1)]\nax1 = sns.distplot(a=s2.Age,bins=34, kde=False, \n                  hist_kws={\"rwidth\":1,'edgecolor':'black', 'alpha':1.0},color='cyan',ax=ax1)\n#ax1.set_title(\"Histogram of Survival, female\")\nax1.legend(['total count', 'Survived count'])\nax1.set_title(\"Histogram of Survival, female\")\n\n\ns3= copy1.loc[copy1.Sex=='male']\nax2 = sns.distplot(a=s3.Age,bins=34, kde=False, \n                  hist_kws={\"rwidth\":1,'edgecolor':'black', 'alpha':1.0},color='lavender',label=\"Age\",ax=ax2)\ns4= s1.loc[(s1.Survived==1)]\nax2 = sns.distplot(a=s4.Age,bins=34, kde=False, \n                  hist_kws={\"rwidth\":1,'edgecolor':'black', 'alpha':1.0},color='orchid',ax=ax2)\n#ax1.set_title(\"Histogram of Survival, female\")\nax2.legend(['total count', 'Survived count'])\nax2.set_title(\"Histogram of Survival, male\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* The portion of blue area is much larger than purple\n* At any range of age, the survival rate of female is much higher than male\n\n### Parch and SibSp vs. Age\n* SibSp is highly correlated with SibSp\n* There's negative correlation between Parch and SibSp vs. Age\n* Group the dataset by pairs of (x=Parch,y=SibSP) value and compare each group's average age\n* Ignore the 'Age' missing value in this part","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"copy3=copy1.loc[copy1.Age.notnull()]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X=[0.0,1.0,2.0,3.0,4.0,5.0,6.0]\nY=[0,1,2,3,4,5]\ndef f(x,y):\n    a=copy3.loc[(copy3.Parch==x)&(copy3.SibSp==y)].Age.mean()\n    return a\nX,Y=np.meshgrid(X,Y)\n#Z=f(X,Y)\nZ=np.zeros((6, 7))\nfor i in range(6):\n    for j in range(7):\n        Z[i][j]=f(X[i][j],Y[i][j])\na= copy3.loc[(copy3.SibSp>=4)].Age.mean()\nfor i in range(2):\n    for j in range(7):\n        Z[i+4][j]=a\n\nZ[0][6]=Z[0][5]\nZ[2][4]=Z[2][5]=Z[2][6]=Z[2][3]\nZ[3][3]=Z[3][4]=Z[3][5]=Z[3][6]=Z[3][2]\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* 3D surface plot and contour plot to visualize the relation among SibSp, Parch and Age\n* Z represents the average Age for certain pairs of (SibSp,Parch) value","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Z = np.cos(X ** 2 + Y ** 2)\nfig= plt.figure(figsize=(10,6))\n#ax = plt.axes(projection='3d')\nax = fig.add_subplot(111, projection='3d')\nsurf=ax.plot_surface(X, Y, Z,cmap='viridis', edgecolor='none')\nfig.colorbar(surf, ax=ax, shrink=0.5, aspect=5)\nax.set_title('Average Age for SibSp and Parch')\nax.set_xlim(0, 6);\nax.set_ylim(5, 0);\nax.set_xlabel('Parch')\nax.set_ylabel('SibSp')\nax.set_zlabel('Age.mean()');\n#plt.show()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nfig = plt.figure(figsize=(8,6))\nax = fig.add_subplot()\n\ncset = ax.contourf(X, Y, Z)\nplt.colorbar(cset)\n\nax.set_title('contour plot');\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Larger SibSp and Parch value implies lower average Age\n* Young people are more likely to be companied by their family member\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Age, Family Size, Fare and Survive","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"* Age and Family size vs Survive\n* Divide all the people into several Age group: 0~18, 18~35, 35~55, 55~80","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"copy3=copy1[['Age','Fare','Survived','SibSp','Parch']]\ncopy3['family_size']=copy3['SibSp']+copy3['Parch']\ndef f (p):\n    if p<= 18: return '0~18'\n    if p>18 and p<35: return '18~35'\n    if p>=35 and p<55: return '35~55'\n    if p>=55: return '55~80'\ncopy3['age_group']=copy3.Age.map(lambda p: f(p))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nsns.set(style=\"darkgrid\")\n#sns.set(style=\"darkgrid\")\nsns.catplot(x='family_size',col='age_group', col_wrap=2,data=copy3,hue=\"Survived\", kind=\"count\",height=8, aspect=.8,palette=['aqua','pink'])\n#sns.catplot(x='Age',y='SibSp',data=copy3,height=8, aspect=.7)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Visualize Age, Fare,Family size and Survived","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure()\nfig, ax = plt.subplots(figsize=(11,6))\nax = fig.add_subplot(111, projection='3d')\nX=copy1['SibSp']+copy1['Parch']\nY=copy1.Age\nZ=copy1.Fare\ng=ax.scatter(X, Y, Z,  c= copy1.Survived,marker='o',cmap='cool')\nplt.colorbar(g)\nax.set_zlim(0, 250);\nax.set_xlabel('Family size')\nax.set_ylabel('Age')\nax.set_zlabel('Fare');\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Fare of Purple dots are higher than blue dots on average\n* Higher Fare price tends to stay alive","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Pclass, Fare vs. Survived\n* First, consider Pclass and Fare which are negatively correlated\n* Fare distribution given a certain Pclass","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"s1=copy1.loc[copy1.Pclass==1].Fare.describe()\ns2=copy1.loc[copy1.Pclass==2].Fare.describe()\ns3=copy1.loc[copy1.Pclass==3].Fare.describe()\npd.concat([s1,s2,s3],axis=1,keys=['Fare|Pclass=1','Fare|Pclass=2','Fare|Pclass=3'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set(style=\"darkgrid\")\nfig, ax1 = plt.subplots(figsize=(16,5))\nax=sns.kdeplot(data=copy1.loc[copy1.Pclass==1]['Fare'],shade=True,color='red')\nax.set_title(\"Fare distribution|Pclass\")\nax=sns.kdeplot(data=copy1.loc[copy1.Pclass==2]['Fare'],shade=True,color='blue')\nax=sns.kdeplot(data=copy1.loc[copy1.Pclass==3]['Fare'],shade=True,color='purple')\nax.legend([\"Fare|Pclass=1\",\"Fare|Pclass=2\",\"Fare|Pclass=3\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n* Acccoring to the above distribution plot and table, we can roughly say that Fare|Pclass=1> Fare|Pclass=2 > Fare|Pclass=3 on average.\n* The Fare|Pclass=1 also has a larger standard error than any other Pclass groups\n* It implies that as Pclass get upper, the ticket price will be more expensive on average","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax1 = plt.subplots(figsize=(12,15))\nsns.swarmplot(x=copy1.Survived,y=copy1.Fare,hue=copy1.Pclass, palette='cool')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.catplot(x=\"Survived\",y=\"Fare\",hue=\"Pclass\",data=copy1,kind='violin',height=8, aspect=2, palette=['violet','turquoise','tomato'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Sex, Pclass, Fare and Survived","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set(style=\"darkgrid\")\nsns.catplot(x=\"Survived\",y=\"Fare\",col=\"Pclass\",data=copy1 ,hue=\"Sex\",height=8, aspect=.7, palette=['violet','turquoise'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Higher Fare price and upper class tends to live ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Pclass,Age,Sex vs Survived","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"g = sns.FacetGrid(copy1,height=5, col=\"Pclass\", row=\"Sex\", margin_titles=True, hue = \"Survived\" )\ng = g.map(sns.distplot, \"Age\",kde=False,bins=15,hist_kws={\"rwidth\":1,'edgecolor':'black', 'alpha':1.0}).add_legend();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Embarked, Fare and Pclass\n* Embarked and Pclass","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"s0=copy1.Embarked.value_counts()\ns1=copy1.loc[copy1.Pclass==1].Embarked.value_counts()\ns2=copy1.loc[copy1.Pclass==2].Embarked.value_counts()\ns3=copy1.loc[copy1.Pclass==3].Embarked.value_counts()\npd.concat([s0,s1/s0*100,s2/s0*100,s3/s0*100],axis=1,keys=['Total','Embarked|Pclass=1 (%)','Embarked|Pclass=2 (%)','Embarked|Pclass=3 (%)'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,12))\nsns.boxplot(x=copy1.Embarked,y=copy1.Fare,hue=copy1.Pclass,palette='cool')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Embarked, Pclass and Fare vs. Survived\n* Embarked and Survived","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,12))\nsns.swarmplot(x=copy1.Embarked,hue=copy1.Survived,y=copy1.Fare,palette='spring')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Embarked S and C have higher survival rate than Embarked Q\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set(style=\"darkgrid\")\nsns.catplot(x=\"Survived\",col=\"Pclass\",data=copy1 ,hue=\"Embarked\",kind='count',height=8, aspect=.7, palette=['gold','orangered','brown'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"------\n## 2.3  Missing Values\nThere are totally 891 entries in the train dataset and 418 entries in the test dataset. However, the variable: Age, Cabin and Embarked in train dataset contain null values and the variables:  Age, Fare,Cabin in test dataset contain null values. Find the best method to deal with missing value problem.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Embarked\n* According to the correlation heatmap, we know that Embarked is highly correlated to Sex, Fare and Pclass\n* We gonna find out the missing values based on these 2 features","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"copy1.loc[copy1.Embarked.isnull()]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* There's only 2 missing values in Embarked\n* They share the same Sex,Pclass and Fare","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"copy1.loc[(copy1.Pclass==1) & (copy1.Sex=='female')].Embarked.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* People from Pclass=1 and Sex= female are more likely to from Embarked=S or C\n* Let's see the Fare description for (Pclass=1 & Sex='female')","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"s1=copy1.loc[(copy1.Pclass==1) & (copy1.Sex=='female')]\ns2=s1.loc[s1.Embarked=='S'].Fare.describe()\ns3=s1.loc[s1.Embarked=='C'].Fare.describe()\npd.concat([s2,s3],axis=1,keys=['Fare|Embarked=S', 'Fare|Embarked=C'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* As the mean of Fare are 99 and 115 for Embarked='S' and Embarked='C' respectively\n* 99 is much closer to 80 than 115\n* Assign Embarked='S' to these 2 missing values","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"* Refill the missing values for new_Embarked in both training and testing dataset","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"copy1['Embarked']=copy1.Embarked.fillna('S')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"label_encoder=LabelEncoder()\ncopy1['new_Embarked']=label_encoder.fit_transform(copy1['Embarked'])\ncopy2['new_Embarked']=label_encoder.transform(copy2['Embarked'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n### Cabin\n* Cabin is highly correlated to Pclass, Fare, Age and Sex\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# make a temporary column to store the first character\nprint('Null Pertcentage of Cabin in tranining dataset : ',str(len(copy1.loc[copy1.Cabin.isnull()])/len(copy1.Cabin)*100),'%')\nprint('Null Pertcentage of Cabin in testing dataset : ',str(len(copy2.loc[copy1.Cabin.isnull()])/len(copy2.Cabin)*100),'%')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* 77% of the Cabin data are null for both dataset\n* Cabin column should be abandoned","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"copy1=copy1.drop(['Cabin','new_Cabin'],axis=1)\ncopy2=copy2.drop(['Cabin','new_Cabin'],axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Age\n* Age is highly correlated to Pclass, SibSp and Parch\n* We can apply a prediction for the missing values based on these features\n* Gonna choose LogisticRegression in this part","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\n# training dataset\nfeature=['Pclass','SibSp','Parch']\ncopy3=copy1.loc[copy1.Age.notnull()]\nx_train=copy3[feature]\n#convert y_train to integer\ny_train=copy3.Age.astype(int)\n\n# prediction\ncopy4=copy1.loc[copy1.Age.isnull()]\nx_test=copy4[feature]\n\nlog=LogisticRegression()\nlog.fit(x_train,y_train)\ny_pred = log.predict(x_test)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# assign the new values to Age column\ncopy1.loc[copy1.Age.isnull(), \"Age\"] = y_pred","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Testing datset\nx_test=copy2.loc[copy2.Age.isnull()][feature]\ny_pred=log.predict(x_test)\ncopy2.loc[copy2.Age.isnull(), \"Age\"] = y_pred","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Fare\n* There's a missing entry in Fare column of testing dataset\n* Fare is highly correlated to Pclass, new_Embarked,new_Sex, SibSP and Parch\n* Apply the random forest regressor to make a prediction","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"copy2.loc[copy2.Fare.isnull()]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# training dataset\nfeature=['Pclass','SibSp','Parch','new_Sex','new_Embarked']\ncopy3=copy2.loc[copy2.Fare.notnull()]\nx_train=copy3[feature]\n#convert y_train to integer\ny_train=copy3.Fare\n\n# prediction\ncopy4=copy2.loc[copy2.Fare.isnull()]\nx_test=copy4[feature]\n\nforest_model=RandomForestRegressor(random_state=1)\nforest_model.fit(x_train,y_train)\ny_pred = forest_model.predict(x_test)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"copy2.loc[copy2.Fare.isnull(), \"Fare\"] = y_pred","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"-----\n# 3. Feature Engineering and Statistical Analysis\nIn this part, we will create some features by applying feature engineering. Then select the useful features among them by statistical analysis to prepare for model prediction.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## 3.1 Features Generation\n### Name\n* The name of passengers also contain title\n* Extract the title from name and treat it as categorical variable","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#[i.split('.')[1] for i in data.Name]\n#for i in range(len(data.Name)):\n    #title = data.Name[i].split('.')[0]\n    #title = title.split(',')[1]\ncopy1['Title']=[n.split('.')[0] for n in copy1.Name]\ncopy1['Title'] = [t.split(',')[1] for t in copy1.Title]\n\ncopy2['Title']=[n.split('.')[0] for n in copy2.Name]\ncopy2['Title'] = [t.split(',')[1] for t in copy2.Title]\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Demonstrate all sorts of title:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.concat([copy1,copy2]).Title.value_counts()\n#copy1.Title.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Convert 'Title' column into category variables\n* There are 18 different kinds of titles in dataset and train dataset only contains 17 of them\n* Fit the Label-Encoding on concatation of train dataset and test datset","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"label_encoder=LabelEncoder()\nlabel_encoder.fit_transform(pd.concat([copy1,copy2])['Title'])\ncopy1['new_Title']=label_encoder.transform(copy1['Title'])\ncopy2['new_Title']=label_encoder.transform(copy2['Title'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Ticket\n* Create a feature Ticket's string length","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"copy1['Ticket_length']=[len(i) for i in copy1.Ticket]\ncopy2['Ticket_length']=[len(i) for i in copy2.Ticket]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### SibSp & Parch\n* SibSp and Parch are both illustrating number of family members\n* Set a new feature Famsize = SibSp + Parch","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"copy1['Famsize']=copy1['SibSp']+copy1['Parch']\ncopy2['Famsize']=copy2['SibSp']+copy2['Parch']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3.2 Statistical Analysis and Feature Selection\n* Draw the Correlation Heatmap based on the features in hand","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# the correlation matrix\ncorr=copy1.corr() \n# mask the upper triangle\nsns.set(style=\"white\")\nplt.figure(figsize=(11,7))\nmask=np.triu(np.ones_like(corr,dtype=np.bool))\n# colour\ncmap=sns.diverging_palette(240,10,n=9)\n# annot to display the value\nsns.heatmap(corr,annot=True,mask=mask,cmap='jet',linewidths=0.6)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Some of the features have almost no correlation with Survived (eg. PassengerId)\n* The correlation magnitude of [PassengerId, Age, SibSp, Parch, Ticket_length, Famsize] and Survived are less then 0.1\n* Apply the Pearson Residual Test to have a better insight\n-----\n#### Pearson's Residual Test\n* For a given factor, The null hypothesis of a feature is that 'The prediction of Survived will not consider this factor'.\n* Use the scipy.stats library to calculate the p-value and compare it with alpha=0.05. If the p-value between the factor and the response is larger than alpha, then this factor does not have a significant level of 95%. The null hypothesis will not be rejected. However, if the p-value is less than alpha, the factor will be rejected.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# scipy.stats to find the p-value and of each factor\n# compare p-value with alpha=0.05 to find out the significance level\n# select the columns for test\nimport scipy.stats as stats\nfrom scipy.stats import chi2_contingency\n\nfeatures= ['PassengerId', 'Pclass','Age','SibSp','Parch','Fare','Embarked','new_Sex','new_Embarked','new_Title','Ticket_length','Famsize']\n# drop the missing value row to have a accurate estimation?\nfor feature in features:\n    table = pd.crosstab(copy1[feature], copy1['Survived'], margins=False)\n    stat, p, dof, expected = stats.chi2_contingency(table)\n    print(\"The p-value of\", feature,\"is: \",p)\n    \n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* The p-value of PassengerId is larger than 0.05.\n* PassengerId does not have a siginificant level of 95% and can be dropped\n* Also the object types feature [Name,Sex,Ticket, Embarked,Title] should be dropped\n* Famsize is the sum of SibSp and Parch. They have high relevancy and we only need one of [Famsize, SibSp&Parch].\n* Obviously, Famsize has a much higher siginificant level than SibSp and Parch, so SibSp and Parch should be abandoned","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"copy1=copy1.drop(['PassengerId','Name','Sex','Embarked','Ticket','Title','SibSp','Parch'],axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"-----\n# 4.Modelling\nApply several prediction model and find the best of them","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## 4.1 Building Models\n* Divide the trainging dataset into 2 groups \n* One group for training and Another group for model testing say valid dataset","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\ny=copy1['Survived']\ncopy3=copy1.drop(['Survived'],axis=1)\nX_train,X_valid,y_train,y_valid = train_test_split(copy3,y,train_size=0.8,test_size=0.2,random_state=0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Logistic Regression","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"l1=LogisticRegression()\nl1.fit(X_train,y_train)\ny_pred=log.predict(X_valid)\nmean_absolute_error(y_valid, y_pred)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Random Forest Regressor","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"forest_model=RandomForestRegressor(random_state=1)\nforest_model.fit(X_train,y_train)\ny_pred=forest_model.predict(X_valid)\nmean_absolute_error(y_valid, y_pred)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Roc Curve\n* Let the sample size be: a+b+c+d\n* Let Y be the real value and $\\hat{Y}$ is the prediction value","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"s1=pd.Series(['a','c'], index=['Y=1','Y=0'])\ns2=pd.Series(['b','d'], index=['Y=1','Y=0'])\npd.concat([s1,s2],axis=1,keys=['$\\hat{Y}$=1', '$\\hat{Y}$=0'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Then we can calculate sensitivity and Specificity:\n* **Sensitivity**: $P(\\hat{Y}=1|Y=1) = \\frac{a}{a+b}$\n* **Specificity**: $P(\\hat{Y}=0|Y=0) = \\frac{d}{c+d}$\n* The ROC curve is plotting **sensitivity** in y-axis and **1-specificity** in x-axis. \n* Concordance index c is the area under ROC curve. The bigger the c, the better the model.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"AIC , BIC and ROC curve","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"#train_data.dropna(axis=0, subset['Survived'],inplace=True)\nfrom sklearn.model_selection import train_test_split\ndata.dropna(axis=0, subset=['Survived'], inplace=True)\ny=data['Survived']\ndata.drop('Survived',axis=1,inplace=True)\n\n# Break off into validation and training dataset\nX_train,X_valid,y_train,y_valid = train_test_split(data,y,train_size=0.8,test_size=0.2,random_state=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# A function to calculate the mean absolute error\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_absolute_error\n\ndef score_dataset(X_train, X_valid, y_train, y_valid):\n    model = RandomForestRegressor(n_estimators=100, random_state=0)\n    model.fit(X_train, y_train)\n    preds = model.predict(X_valid)\n    return mean_absolute_error(y_valid, preds)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Method1: Drop the missing value column\ncols_with_missing = [col for col in X_train.columns\n                    if X_train[col].isnull().any()]\nreduced_X_train = X_train.drop(cols_with_missing,axis=1)\nreduced_X_valid = X_valid.drop(cols_with_missing,axis=1)\n\nprint(\"MAE for dropping the missing valus column: \", score_dataset(reduced_X_train,reduced_X_valid, y_train, y_valid))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}